<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <!--<meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>-->
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <!--<meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">-->
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <!--<meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">-->
    <!-- Keywords for your paper to be indexed by-->
    <!--<meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">-->
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Quadruple Semantic Align Net (SAN-QUAD)</title>
    <!--<link rel="icon" type="image/x-icon" href="./static/images/favicon.ico">-->
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="./static/css/style.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
  </head>
  <body>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                Enhancing Ground-to-Aerial Image Matching for Visual
                Misinformation Detection Using Semantic Segmentation
              </h1>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://emamule.github.io/" target="_blank"
                    >Emanuele Mule</a
                  ><sup>*</sup>,</span
                >
                <span class="author-block">
                  <a
                    href="https://it.linkedin.com/in/matteo-pannacci"
                    target="_blank"
                    >Matteo Pannacci</a
                  ><sup>*</sup>,</span
                >
                <span class="author-block">
                  <a href="https://alighasemi78.github.io/" target="_blank"
                    >Ali Ghasemi Goudarzi</a
                  ><sup>*</sup>,</span
                >
                <span class="author-block">
                  <a
                    href="https://www.linkedin.com/in/francesco-pro/"
                    target="_blank"
                    >Francesco Pro</a
                  ><sup></sup>,</span
                >
                <span class="author-block">
                  <a
                    href="https://it.linkedin.com/in/lorenzopapa97-5"
                    target="_blank"
                    >Lorenzo Papa</a
                  ><sup></sup>,</span
                >
                <span class="author-block">
                  <a
                    href="https://it.linkedin.com/in/lucamaiano"
                    target="_blank"
                    >Luca Maiano</a
                  ><sup></sup>,</span
                >
                <span class="author-block">
                  <a
                    href="https://sites.google.com/diag.uniroma1.it/ireneamerini"
                    target="_blank"
                    >Irene Amerini</a
                  ><sup></sup
                ></span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <br />
                  *: Equal Contribution
                  <br />
                  WACV 2025
                </span>
                <!--<span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>-->
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Arxiv PDF link -->
                  <span class="link-block">
                    <a
                      href="https://arxiv.org/pdf/2502.06288"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf" style="color: orangered"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Supplementary PDF link -->
                  <!-- <span class="link-block">
                    <a
                      href="https://drive.google.com/file/d/17W9VEPMneRlb6igtSxa--Xh4fSZs3RS_/view?usp=sharing"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-folder" style="color: ffffff"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a
                      href="https://github.com/MatteoPannacci/SemanticAlignNet-QUAD"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/2502.06288"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Teaser video-->
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <h2 class="subtitle has-text-centered">
            <span class="methodname">Quadruple Semantic Align Net</span> model
            is a new contribution idea for the Ground-to-Aerial Image Matching
            task.
          </h2>
        </div>
      </div>
    </section>
    <!-- End teaser video -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                The recent development of generative AI techniques, which has
                increased the online diffusion of altered images and videos,
                raises concerns about the credibility of digital media
                accessible on the Internet and shared by information channels
                and social networks. Domains that rely on this data, such as
                journalism, forensic analysis, and earth observation, suffer
                because of this problem. At this aim, being capable of
                geolocating a non-geo-tagged ground-view image without external
                information, like GPS coordinates, is becoming crucial. This
                study addresses the challenge of linking a ground-view image, on
                different FoV values, to its corresponding satellite image
                without relying on GPS data. A novel four-stream Siamese-like
                architecture,
                <span class="methodname"
                  >Quadruple Semantic Align Net (SAN-QUAD)</span
                >, was introduced to achieve this.
                <span class="methodname">SAN-QUAD</span> expands previous SOTA
                methods, leveraging semantic segmentation applied to both ground
                and satellite images. The obtained results on the CVUSA dataset
                show notable improvements, up to 9.8%, over previous methods
                when tested across all available fields of view (FoV).
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->

    <!-- Image carousel -->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- Your image here -->
              <img src="./static/images/figure1_8.png" alt="Pipeline" />
              <h2 class="subtitle has-text-centered">
                Example of the ground-to-aerial matching problem. The query
                ground-view image is matched to the polar transformed aerial
                image
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="./static/images/model.png" alt="Model Architecture" />
              <h2 class="subtitle has-text-centered">
                The architecture is composed of four branches, two for the
                ground viewpoint and two for the satellite one. Each branch
                takes as input either an RGB image or a semantic segmentation
                mask and produces a feature volume. The volumes relative to the
                same viewpoint are then combined into the final feature
                representations which are compared to obtain the most likely
                orientation and perform the matching.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End image carousel -->

    <!-- Youtube video -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <!-- Paper video. -->
          <h2 class="title is-3">Video Presentation</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="publication-video">
                <!-- Youtube embed code here -->
                <iframe
                  width="560"
                  height="315"
                  src="https://www.youtube.com/embed/oABNgMUd19g?si=GqElAwQ4uLpx7Iu3"
                  title="YouTube video player"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  referrerpolicy="strict-origin-when-cross-origin"
                  allowfullscreen
                ></iframe>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End youtube video -->

    <!-- Results table -->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <!-- Paper video. -->
          <h2 class="title is-3">Results</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="content has-text-centered">
                <p>
                  We created a subset of the CVUSA dataset on which we trained
                  and tested SAN-QUAD model.
                </p>
              </div>
              <div class="publication-video">
                <img src="./static/images/results.png" alt="Results" />
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End results table -->

    <!--BibTex citation -->
    <section class="section hero is-light" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <p class="content has-text-justified">
          Emanuele Mule*, Matteo Pannacci*, Ali Ghasemi Goudarzi*, Francesco
          Pro, Lorenzo Papa, Luca Maiano, and Irene Amerini. Enhancing
          ground-to-aerial image matching for visual misinformation detection
          using semantic segmentation. In
          <i
            >Proceedings of the Winter Conference on Applications of Computer
            Vision (WACV) Workshops</i
          >, pages 795-803, February 2025.
        </p>
        <pre class="selectable"><code>
@inproceedings{mule2025enhancing,
    title={Enhancing Ground-to-Aerial Image Matching for Visual Misinformation Detection Using Semantic Segmentation},
    author={Mule, Emanuele and Pannacci, Matteo and Goudarzi, Ali Ghasemi and Pro, Francesco and Papa, Lorenzo and Maiano, Luca and Amerini, Irene},
    booktitle={Proceedings of the Winter Conference on Applications of Computer Vision},
    pages={795--803},
    year={2025}
}

</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the <a
                  href="https://nerfies.github.io"
                  target="_blank"
                  >Nerfies</a
                > project page. <br />
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
</html>
